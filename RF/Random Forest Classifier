#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
get_ipython().run_line_magic('matplotlib', 'inline')
import warnings
warnings.filterwarnings('ignore')


# In[29]:


df = pd.read_csv('INIT.csv',sep=';')
df = df.drop(["ID"],1)
df.head()


# In[30]:


print(df.columns)
print(df.shape)


# In[31]:


df.info()


# In[32]:


df.isnull().sum()


# In[33]:


correlation = df.corr()
plt.subplots(figsize=(30,10))
sns.heatmap(correlation, square=True, annot=True, fmt=".1f" )


# In[34]:


X = df.iloc[:,:-1].values
y = df.iloc[:,-1].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 1)


# In[35]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_score

rfc = RandomForestClassifier(n_estimators = 100,criterion = 'entropy',random_state = 0)
rfc.fit(X_train,y_train)
y_pred = rfc.predict(X_test)
roc = roc_auc_score(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
results = pd.DataFrame([['Random tree Classifier', acc,prec,roc]],
columns = ['Model','Accuracy','Precision','ROC'])
results.to_csv('INIT_Performance.csv',index=False)


# In[36]:


from sklearn.metrics import confusion_matrix
plt.figure(figsize=(8,6))
ConfMatrix = confusion_matrix(y_test,y_pred)
sns.heatmap(ConfMatrix,annot=True, cmap="Blues", fmt="d", 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'])
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title("Confusion Matrix - Random Forest")


# In[37]:


from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))


# In[38]:


import sklearn.metrics as metrics

probrf = rfc.predict_proba(X_test)
predsrf = probrf[:,1]
fprrf, tprrf, thresholdrf = metrics.roc_curve(y_test, predsrf)
roc_aucrf = metrics.auc(fprrf, tprrf)
plt.title('ROC Curve')
plt.plot(fprrf, tprrf, 'b', label = 'AUCrf = %0.2f' % roc_aucrf)
plt.legend(loc = 'lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([0,1])
plt.ylim([0,1])
plt.ylabel('True Positive Rate rf')
plt.xlabel('False Positive Rate rf')
plt.show()


# In[39]:


importances = rfc.feature_importances_
indices = np.argsort(importances)
features = df.columns

plt.figure(figsize=(10,10))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()


# In[41]:


features = features.drop(['default.payment.next.month'])


# In[43]:


feature_importances = pd.Series(importances,features)
feature_importances.sort_values(ascending=False)
feature_importances.to_csv('INIT_Features.csv',index=False)
